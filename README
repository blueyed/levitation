This is Levitation, a project to convert MediaWiki database dumps into Git
repositories. MediaWiki is the free software that is powering Wikipedia.

System Requirements:

 - Python 2.6 (2.5 might work as well, please report back)
 - a reasonably current Git installation (1.5 or higher)
 - lots of disk space (see below for a definition of “lots of”)
 - 1 GB of RAM or more
 - the faster the CPU, the better (dual core recommended)
 - Unix OS strongly recommended for performance reasons (but Windows users,
   please report back how painful it was!)

User Requirements:

 - basic to intermediate knowledge of Git

Since the code is currently being restructured a bit, we don’t have current
numbers for speed and disk usage. As a rule of thumb: You should be able to
import a “smaller” Wikipedia (like {pdc,bar,vo}.wikipedia.org) on your laptop.

A word about the RAM requirements: Currently, the dump is imported one revision
at a time. RAM usage should therefore be well below 1 GB. However, since we’re
importing XML, and importing XML involves all kinds of enterprisey I-don’t-
care-about-resources tool overhead, I won’t guarantee anything. Possibly my
code or Python itself is memleaking as well. Once your system starts swapping,
data throughput will sink remarkably fast, so try to avoid memory shortage.
Hint: If you’re in the middle of a thrashing, unresponsive system, don’t Ctrl-C
Python, but kill it using its PID. That’s _way_ faster.

Now, disk usage. First of all, there are four distinct places on your mass
storage involved in the import process:

 (a) the input file
 (b) the output repo
 (c) Levitation’s metadata files
 (d) your swap file

If you can, place these on separate physical disks to improve speed. If you
don’t have that many disks, at least try to separate (b) and (c). Keeping (d)
separate isn’t too important as well: You’re probably lost anyway when your
machine starts swapping.

Where (b) is placed is determined by the GIT_DIR variable you pass to
git-fast-import (see below where we discuss the invocation). The (c) files are
by default placed as “.import-*” in the current working directory, so “cd” to
where you want them first or use the -{M,C,U,P} parameters to put them
somewhere else.

To allow fast random access to the metadata files, Levitation uses a binary
format with fixed-length data sets. The file size is therefore determined by
the product of entry_length * max_value, where max_value is the highest ID of
some kind that appears in the input file. The factors for each meta file are:

 meta:  17 bytes per revision  (linking the three other files together)
 comm: 257 bytes per revision  (storing “edit summary” lines)
 user: 257 bytes per user      (storing the user name)
 page: 257 bytes per page      (storing the page title)

Again, it doesn’t matter how many revisions/users/pages you have in the file,
it’s their maximum ID number that counts. When importing a MediaWiki, you can
read the revision, user and page count on its “Special:Statistics” page.
Alternatively you may use contrib/getmaxids.sh to retrieve those values from
your XML dump.

Example: The highest page ID in your dump is 31337, the highest revision ID is
424242 and the highest user ID is 1701. Then, the meta files will need:

        meta            comm            user            page
   (17 * 424242) + (257 * 424242) + (257 * 1701) + (257 * 31337)
 =    7212114    +    109030194   +    437157    +    8053609
 = 124733074 bytes (~118 MiB)

Note that these are just the metadata requirements. These files can be deleted
after importing. Not deleting them does not improve the speed of future runs,
as they are recreated on every invocation.

The Git repository itself usually needs a lot more space. Expect at least(!)
the size of the bz2 compressed dump. The actual size will most likely be
between 50% and 300% of that value. Yes, that’s pretty vague.

Also, note that the invocation demoed below creates a bare repository (i.e. one
without a checked out working tree). You generally want it like that. When you
decide to check out the finished repo, make sure the place where you check it
out has enough free space and (probably more important) enough free inodes.


Usage:

Currently, Levitation’s main tool is import.py, which is essentially a filter.
It takes the XML dump data on stdin and outputs a datastream suitable for
consumption by git-fast-import(1). You may use all kinds of Unix magic to
produce a valid input stream. This allows you for example to decompress a bz2
compressed file on the fly while reading. Therefore all of the following
examples are just that: examples. They show some common usage scenarios, your
own command line may vary.

You may import dumps from any MediaWiki in the world, but most of you will
probably want to play around with Wikipedia dumps. You can recent ones at:
http://download.wikimedia.org/backup-index.html

The pages-meta-history.xml file (with whatever compression) is what we want.
(In case you’re wondering: Wikimedia does not offer content SQL dumps anymore,
and there are no full-history dumps for en.wikipedia.org because of its size.)
It includes all pages in all namespaces and all of their revisions.

Alternatively, you may use a MediaWiki’s “Special:Export” page to create an XML
dump of certain pages.


Things that work:

 - Read a Wikipedia XML full-history dump and output it in a format suitable
   for piping into git-fast-import(1). The resulting repository contains one
   file per page. All revisions are available in the history. There are some
   restrictions, read below.

 - Use the original modification summary as commit message.

 - Read the Wiki URL from the XML file and set user mail addresses accordingly.

 - Use the author name in the commit instead of the user ID.

 - Store additional information in the commit message that specifies page and
   revision ID as well as whether the edit was marked as “minor”.

 - Use the page’s name as file name instead of the page ID. Non-ASCII
   characters and some ASCII ones will be replaced by “.XX”, where .XX is their
   hex value.

 - Put pages in namespace-based subdirectories.

 - Put pages in a configurably deep subdirectory hierarchy.

 - Use command line options instead of hard-coded magic behavior. Thanks to
   stettberger for adding this.

 - Use a locally timezoned timestamp for the commit date instead of an UTC one.


Things that are still missing:

 - Allow IPv6 addresses as IP edit usernames. (Although afaics MediaWiki itself
   cannot handle IPv6 addresses, so we got some time.)


Things that are strange:

 - Since we use subdirectories, the Git repo is no longer larger than the
   uncompressed XML file, but instead about 30% of it. This is good. However,
   it is still way larger than the bz2 compressed file, and I don’t know why.


Things that are cool:

 - “git checkout master~30000” takes you back 30,000 edits in time — and on my
   test machine it only took about a second.

 - The XML data might be in the wrong order to directly create commits from it,
   but it is in the right order for blob delta compression: When passing blobs
   to git-fast-import, delta compression will be tried based on the previous
   blob — which is the same page, one revision before. Therefore, delta
   compression will succeed and save you tons of storage.


Example usage:

Please note that there’s the variable IMPORT_MAX, right at the beginning of
import.py. By default it’s set to 100, so Levitation will only import 100
pages, not more. This protects you from filling your disk when you’re too
impatient. ;) Set it to -1 when you’re ready for a “real” run.

This will import the pdc.wikipedia.org dump into a new Git repository “repo”:

  rm -rf repo; git init --bare repo && \
    ./import.py < ~/pdcwiki-20091103-pages-meta-history.xml | \
    GIT_DIR=repo git fast-import | \
    sed 's/^progress //'

Execute “import.py --help” to see all available options.


Storage requirements:

“maxrev” be the highest revision ID in the file.

“maxpage” be the highest page ID in the file.

“maxuser” be the highest user ID in the file.

The revision metadata storage needs maxrev*17 bytes.

The revision comment storage needs maxrev*257 bytes.

The author name storage needs maxuser*257 bytes.

The page title storage needs maxpage*257 bytes.

Those files can be deleted after an import.

Additionally, the content itself needs some space. My test repo was about 15%
the size of the uncompressed XML, that is about 300% the size of the bz2
compressed XML data (see “Things that are strange”).

Note that if you want to check out a working copy, the filesystem it will be
living on needs quite a few free inodes. If you get “no space left on device”
errors with plenty of space available, that’s what hit you.


Contacting the author:

This monster is written by Tim “Scytale” Weber. It is an experiment, whether the
current “relevance war” in the German Wikipedia can be ended by decentralizing
content.

Find ways to contact me on http://scytale.name/contact/, talk to me on Twitter
(@Scytale) or on IRC (freenode, #oqlt).

Get the most up-to-date code at http://scytale.name/proj/levitation/.

Please also read the FAQ if you have any questions, including “how can I help”.


This whole bunch of tasty bytes is licensed under the terms of the WTFPLv2.
